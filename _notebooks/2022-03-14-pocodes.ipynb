{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e257faee-3b81-4575-ae8b-ec570bf861f5",
   "metadata": {},
   "source": [
    "# \"POCODES: POssible COllision DEtection System\"\n",
    "> \"Part [2/3] of my Road to Masters story\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- image: images/ipynb/colision_predictions1.png\n",
    "- comments: false\n",
    "- author: Giaco Stantino\n",
    "- categories: [computer vision, machine learning]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- permalink: /blog/pocodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be308a4-90be-4639-9aba-357e14c6c1ec",
   "metadata": {},
   "source": [
    "##  <center> Intro </center>\n",
    "\n",
    "\"Contrary to traditional beliefs, high-level reasoning requires\n",
    "low computing power, but low-level perception and abilities\n",
    "motor skills require enormous computing power.\"<br>~Moravec paradox\n",
    "\n",
    "Indeed, computers have learned chess against people earlier than they have learned to recognize items in photos or in the vicinity of the car. The following post is based on the project and report I done for my univeristy project. I had to design aglorithm and model that would spot the possibile collisions in the frame of the video. Fun fact: this is the project that convinced me that ML is what I'd like consider for my master's thesis and maybe for coming years.\n",
    "\n",
    "##  <center> Prelude </center>\n",
    "\n",
    "The growing number of vehicles on the road has visible consequences, including congested streets in densely populated cities and an increasing number of car accidents from year to year. A 2019 World Health Organization (WHO) report shows that 1.25 million people die in road accidents each year and millions more are injured, with nearly 49% of pedestrians and cyclists. Moreover, it is estimated that road accidents will be the fifth leading cause of death in the world by 2030 if preventive measures are not taken.\n",
    "\n",
    "At the same time, there is an increase in the use of drones and the development of related industries. By 2025, the size of the drone services market is expected to grow to $ 63.6 billion. In addition, in 2021, deliveries of consumer drones are expected to reach 29 million. At this point, attention should be paid to the presence of vision systems in most models available on the market.\n",
    "\n",
    "The use of artificial intelligence and computer vision for the image obtained from the drone's camera to detect potential collisions may significantly accelerate the process of providing information about the accident. The time it takes for victims to wait for help could be shortened. Such a system should be fully automatic and operate with high reliability.\n",
    "\n",
    "##  <center> Object Detection </center>\n",
    "\n",
    "Object detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO and SSD. Two-stage methods prioritize detection accuracy, and example models include Faster R-CNN and Mask R-CNN.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_objectdetection.png\" width=500></center>\n",
    "\n",
    "Algorithms produce a list of object categories present in the image along with an axis-aligned *bounding box* indicating the position and scale of every instance of each object category. In fact, there are two types of *boxes* that you should know:\n",
    "\n",
    " - **The bounding box** is used to describe the spatial location of an object, as shown in the above photo\n",
    " - **The ground-truth box** is used on training data to mark the object\n",
    "\n",
    " \n",
    "\n",
    "## <center> Model used - SSD </center>\n",
    "\n",
    "In 2015, Wei Liu et al. published the work [\"SSD: Single Shot MultiBox Detector\"](https://arxiv.org/abs/1512.02325). This algorithm uses boxes of different sizes to recognize objects of different types in any image. Thanks to the use of this technique, it is possible to recognize large and small objects in the same image.\n",
    "\n",
    "SSD has two components: a  **backbone**  model and  **SSD head**.  _Backbone_  model usually is a pre-trained image classification network as a feature extractor. This is typically a network like ResNet trained on ImageNet from which the final fully connected classification layer has been removed. We are thus left with a deep neural network that is able to extract semantic meaning from the input image while preserving the spatial structure of the image albeit at a lower resolution.  The *SSD head*  is just one or more convolutional layers added to this backbone and the outputs are interpreted as the bounding boxes and classes of objects in the spatial location of the final layers activations. In the figure below, the first few layers (white boxes) are the backbone, the last few layers (blue boxes) represent the SSD head.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_ssdmodel.png\" width=700></center>\n",
    "\n",
    "The algorithm creates a mesh and checks for each location whether the object is potentially there.  Then it selects the correct proportions of the default boundaries for the searched object. In the SSD model, this stage is accomplished by the standard VGG-16 convolutional network model.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_ssdboxes.png\" width=700></center>\n",
    "\n",
    " The figure above from the left shows GT (ground-truth) boxes where two animal objects (dog and cat) were found, the center shows an 8 x 8 map and finding a smaller element, and the right shows the location coordinates (loc) and values ​​representing the trust of the found object on the basis of the compared features (conf).\n",
    "\n",
    "## <center> Collision detection algorithm </center>\n",
    "\n",
    "The SSD model is used to predict bounding boxes of objects in the frame. The risk of a potential accident is detected by checking the **IoU** (Intersection over Union) factor for each pair of objects visible in the frame. This concept is sufficient for frames with a limited number of objects, but might not work in places with extremely high density of objects, such as passersby and cars in Times Square, New York\n",
    "\n",
    "The **IoU** factor is used to determine the degree to which two objects overlap, namely their predicted boundaries. It is calculated by dividing the intersection of the surfaces bounded by the boundaries by the sum of both surfaces, as shown in the following illustration.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_iou.png\" width=700></center>\n",
    "  \n",
    "## <center> Results </center>\n",
    "\n",
    "The SSD300 neural network model was used to detectthe objects. The database was recordings from Stanford University with annotations already prepared. An example frame below.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_annotations.png\" width=700></center>\n",
    "\n",
    "The analysis was carried out for single frames of films available in the above-mentioned database.  624 frames were selected pseudo-randomly for the training set and 132 for the validation set. Such a selection was aimed at diversifying the set, because the two consecutive frames of a film with a frequency of 30 fps do not differ much from each other. \n",
    "\n",
    "### Predictions\n",
    "\n",
    "Trained model allows to recognize objects of various classes:\n",
    "Biker, Skateboarder and Car. Examples of frames with prediction plotted from two locations are presented below.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_predictions1.png\" width=700></center>\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_predictions2.png\" width=700></center>\n",
    "\n",
    "AP (Average Precision) was used to assess the quality, which is an indicator showing the effectiveness of predictions in percent, i.e. the ratio of correct predictions to all predictions made. The average precision value for both scenes was 73.51%, which is not a sufficient enough for the stable and reliable  collision detection system.\n",
    "\n",
    "### Collisions\n",
    "\n",
    "The risk of a potential accident was detected by checking the IoU factor for each pair of objects predicted in the frame. For the purposes of the project, it was decided that the IoU factor must be greater than 0.3 to detect collision.\n",
    "\n",
    "<center><img src=\"https://giacostantino.com/images/ipynb/colision_colisions.png\" width=700></center>\n",
    "\n",
    "We can see  possible collisions in the picture above. The next step in improving considered detection system could be to analyze whether the drop in speed is large enough between suspected objects to confirm the collision.\n",
    "\n",
    "## <center> Summary </center>\n",
    "\n",
    "The project discusses a collision detection system based on computer vision solutions using artificial neural networks from recent years. A system was obtained that recognizes objects of various classes of road users and checks the locations of potential collisions. The average effectiveness of the recognition of objects was at the level of 75.51% which is not a sufficient value. Finally, a solution for detecting collisions of detected objects using the IoU coefficient was proposed.\n",
    "\n",
    "This sums up the blog post. As you can see, the project was not an unbelievable success... in fact it was not successful at all. Well, at least when it comes to the results. Despite that, it managed to ignite my curiosity. I can't wait to post the last part of this saga, which is my master's thesis: video object segmentation.\n",
    "\n",
    "P.S. Sorry for that clickbait title, POCODES :laugh:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolioproject",
   "language": "python",
   "name": "portfolioproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
